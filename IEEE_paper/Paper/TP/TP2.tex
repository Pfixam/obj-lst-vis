\subsection{Creating objects list of camera data}\label{C}
\subsubsection{Object detection and preparation}
The first problem to deal with is to detect any possible object in every given frame. For that PyImageSearch published an useful version of a detection algorithm called \ac{YOLO} \cite{Yolo}. 
Changes in the code had to be made to use multiple frames instead of just one saved image stored on disk.
These frames of the simulation environment can be generated by in-game sensors like RGB and depth camera. It is necessary to place those cameras at the same spot on the ego-vehicle to collect comparable images without any errors by considering angular misalignment. As a result, the camera sensors will create images of 32-bit BGRA colors to work with. 
Because the \ac{YOLO} algorithm needs at least 0.35 seconds on every tested hardware the tick rate has to be synchronized and reduced to 0.5 seconds for both sensors.

To generate predominantly \ac{TP} cases, the confidence value of \ac{YOLO} is set to 0.7 and the threshold value to 0.6. Furthermore, tests in CARLA have resulted in \ac{FP} cases where objects such as umbrellas were detected. To exclude these cases, all irrelevant object classes are filtered out in advance. The bounding boxes are not used as usual to display them in the frame, but the pixel coordinates of the bounding boxes are used. These are composed of a \textit{x} and \textit{y} coordinate, as well as a width \textit{w} and height \textit{h} to determine the location of the detected object in the frame. In addition, the confidence and name of the detected objects are also used. 
\subsubsection{Data processing}
The resulting pixel coordinates of the bounding boxes can be used to cut out a frame of the specific object from the image. This is necessary to filter unsuitable pixel values and general noise in the image with an adaptive threshold filter. This filter also generates a black and white image that makes it possible to detect the silhouette of the given object. An evaluation of the blackened pixels and their resulting distances gives an approximate idea of the location in the simulation environment.
Therefore, the CARLA development team provides a formula (\cref{eq:distance}) to calculate the distance by using the color values of certain pixels of the depth image seen in \cref{fig:depth image} \cite{distance}. 

\begin{equation}
	\label{eq:distance}
	\begin{aligned}
		& (R, G, B) = v_{Pixel}\\
		& d_{norm} = (R+G*256+B*256*256)\,/\,(256*256*256-1)\\
		& d_{direct} = 1000 * d_{norm}\\
	\end{aligned}
\end{equation}
\begin{table}[!h]
	\begin{center}
		\begin{tabular}{l c l}
			$v_{Pixel}$ & = & color values of specific pixel\\
			$d_{norm}$ & = &  normalized displacement\\
			$d_{disp}$ & = & displacement of camera sensor and object\\
		\end{tabular}
	\end{center}
\end{table}


\begin{figure}[b]
	\centering
	\fbox{\includegraphics[width=0.4\textwidth]{depthimage.png}}
	\caption{Image codifying depth value via RGB color space}
	\label{fig:depth image}
\end{figure}

This makes it possible to determine not only the direct distance of any visible object, but also the length of the object itself. By processing the calculated distances in a frame it is possible to estimate the length. To get accurate results at this early stage the object has to be fully visible. Otherwise the length is an approximated estimation. 
This value is added to the previously calculated distance to obtain the center of an object. To get the final position in the simulation environment taking into account the rotation of an object, the yaw angle must be considered, too.\\

\subsubsection{Object Tracker}
Over time, new objects become visible and some disappear. For now, the detection algorithm cannot track them. In addition, depending on the movement, objects are not detected in the same order. To keep track of all objects it is necessary to identify already detected ones in the following frames. To do so, all bounding boxes must get connected with an unique \ac{ID}. For this purpose a function made available by PyImageSearch is used \cite{Tracker}. This tracker was chosen for the reason that it is just necessary to provide coordinates of the bounding boxes. The code recognizes the movement and links an ID to a specific object in the bounding box.\\