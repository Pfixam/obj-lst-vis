Camera data and its camera-based algorithms are increasingly being used to make people, vehicles, objects and buildings visible for automated vehicles. With the help of these algorithms, the raw camera data can be evaluated regarding to various criteria. Important distinctions are the classification, dimensions, distance, alignment and the relative velocity of the detected objects in relation to the ego-vehicle \cite{Aeberhard}. \ac{YOLO} is one of the most effective real-time object detection algorithms and offers all of these functions \cite{knuthwebsite}.
Due to the fact that new driving functions usually are validated on a proving ground under controlled conditions, environment simulation software such as CARLA are used instead for developing, training and validating driving systems \cite{Gap}. CARLA is an open-source urban driving simulator for autonomous driving research which supports flexible sensor suites, user scripts and full control of all static and dynamic actors and maps \cite{Dosovitskiy17}. This offers the possibility to validate the camera data sets using different kinds of camera-based algorithms for object detection or in addition to train them. To evaluate the \ac{GT} data with the calculated algorithm objects, \ac{ROS} offers the possibility of sending the respective data streams using objects lists and evaluating them with a \ac{RVIZ} \cite{ROS}.
In this work, an autonomous driving environment model visualization based on an objects lists level is presented by using a NCAP test scenario for testing automated driving systems. An analysis is made based on the \ac{YOLO} camera object detection algorithm compared to the \ac{GT} Data directly generated from the environment simulation software model. \Cref{fig:Overview} illustrates the basic process flows for generating all data, visualizing and validating the results.
